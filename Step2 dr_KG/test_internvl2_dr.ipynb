{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data2/renyw/InstallationPackage/anaconda3/envs/internvl1.2/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "def split_model(model_name):\n",
    "    device_map = {}\n",
    "    world_size = torch.cuda.device_count()\n",
    "    num_layers = {\n",
    "        'InternVL2-1B': 24, 'InternVL2-2B': 24, 'InternVL2-4B': 32, 'InternVL2-8B': 32,\n",
    "        'InternVL2-26B': 48, 'InternVL2-40B': 60, 'InternVL2-Llama3-76B': 80}[model_name]\n",
    "    # Since the first GPU will be used for ViT, treat it as half a GPU.\n",
    "    num_layers_per_gpu = math.ceil(num_layers / (world_size - 0.5))\n",
    "    num_layers_per_gpu = [num_layers_per_gpu] * world_size\n",
    "    num_layers_per_gpu[0] = math.ceil(num_layers_per_gpu[0] * 0.5)\n",
    "    layer_cnt = 0\n",
    "    for i, num_layer in enumerate(num_layers_per_gpu):\n",
    "        for j in range(num_layer):\n",
    "            device_map[f'language_model.model.layers.{layer_cnt}'] = i\n",
    "            layer_cnt += 1\n",
    "    device_map['vision_model'] = 0\n",
    "    device_map['mlp1'] = 0\n",
    "    device_map['language_model.model.tok_embeddings'] = 0\n",
    "    device_map['language_model.model.embed_tokens'] = 0\n",
    "    device_map['language_model.output'] = 0\n",
    "    device_map['language_model.model.norm'] = 0\n",
    "    device_map['language_model.lm_head'] = 0\n",
    "    device_map[f'language_model.model.layers.{num_layers - 1}'] = 0\n",
    "\n",
    "    return device_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "def build_transform(input_size):\n",
    "    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD\n",
    "    transform = T.Compose([\n",
    "        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=MEAN, std=STD)\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n",
    "    best_ratio_diff = float('inf')\n",
    "    best_ratio = (1, 1)\n",
    "    area = width * height\n",
    "    for ratio in target_ratios:\n",
    "        target_aspect_ratio = ratio[0] / ratio[1]\n",
    "        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
    "        if ratio_diff < best_ratio_diff:\n",
    "            best_ratio_diff = ratio_diff\n",
    "            best_ratio = ratio\n",
    "        elif ratio_diff == best_ratio_diff:\n",
    "            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
    "                best_ratio = ratio\n",
    "    return best_ratio\n",
    "\n",
    "def dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):\n",
    "    orig_width, orig_height = image.size\n",
    "    aspect_ratio = orig_width / orig_height\n",
    "\n",
    "    # calculate the existing image aspect ratio\n",
    "    target_ratios = set(\n",
    "        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n",
    "        i * j <= max_num and i * j >= min_num)\n",
    "    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
    "\n",
    "    # find the closest aspect ratio to the target\n",
    "    target_aspect_ratio = find_closest_aspect_ratio(\n",
    "        aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n",
    "\n",
    "    # calculate the target width and height\n",
    "    target_width = image_size * target_aspect_ratio[0]\n",
    "    target_height = image_size * target_aspect_ratio[1]\n",
    "    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
    "\n",
    "    # resize the image\n",
    "    resized_img = image.resize((target_width, target_height))\n",
    "    processed_images = []\n",
    "    for i in range(blocks):\n",
    "        box = (\n",
    "            (i % (target_width // image_size)) * image_size,\n",
    "            (i // (target_width // image_size)) * image_size,\n",
    "            ((i % (target_width // image_size)) + 1) * image_size,\n",
    "            ((i // (target_width // image_size)) + 1) * image_size\n",
    "        )\n",
    "        # split the image\n",
    "        split_img = resized_img.crop(box)\n",
    "        processed_images.append(split_img)\n",
    "    assert len(processed_images) == blocks\n",
    "    if use_thumbnail and len(processed_images) != 1:\n",
    "        thumbnail_img = image.resize((image_size, image_size))\n",
    "        processed_images.append(thumbnail_img)\n",
    "    return processed_images\n",
    "\n",
    "def load_image(image_file, input_size=448, max_num=12):\n",
    "    image = Image.open(image_file).convert('RGB')\n",
    "    transform = build_transform(input_size=input_size)\n",
    "    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "    pixel_values = [transform(image) for image in images]\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    return pixel_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '6,7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 17/17 [00:44<00:00,  2.61s/it]\n"
     ]
    }
   ],
   "source": [
    "path = \"/data2/renyw/PythonWorkspace/KnowledgeGraph/InternVL2-40B\"\n",
    "device_map = split_model('InternVL2-40B')\n",
    "model = AutoModel.from_pretrained(\n",
    "    path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    use_flash_attn=True,\n",
    "    trust_remote_code=True,\n",
    "    device_map=device_map).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True, use_fast=False)\n",
    "generation_config = dict(max_new_tokens=1024, do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Hello, who are you?\n",
      "Assistant: I am an AI assistant whose name is InternVL, developed jointly by Shanghai AI Lab and SenseTime.\n"
     ]
    }
   ],
   "source": [
    "# pure-text conversation (纯文本对话)\n",
    "question = 'Hello, who are you?'\n",
    "response, history = model.chat(tokenizer, None, question, generation_config, history=None, return_history=True)\n",
    "print(f'User: {question}\\nAssistant: {response}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 48\u001b[0m\n\u001b[1;32m     45\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mYou are an experienced ophthalmologist. You are given a text about ophthalmic diseases and some entities in the text. Your task is to select only the single most relevant entity directly associated with ophthalmic diseases from the entities that have the same \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mname\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m and create an ophthalmology knowledge graph according to these entities. The knowledge graph should only include nodes that represent significant ophthalmic entities such as diseases, key symptoms, clinical signs, treatments, anatomical structures specifically related to ophthalmic conditions, and exclude non-specific descriptors such as general fluid accumulation or other non-specific medical terms. The relationships and actions between these nodes are represented as edges. You will respond with a knowledge graph in the given JSON format: [\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mentity\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m : \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mEntity_name\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mcui\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mEntity_CUI\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mconnections\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m : [\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mentity\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m : \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mConnected_entity\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mcui\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mEntity_CUI\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mrelationship\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m : \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mRelationship_with_connected_entity\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mentity\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m : \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mConnected_entity\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mcui\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mEntity_CUI\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mrelationship\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m : \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mRelationship_with_connected_entity\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;124m]. Keep the \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mRelationship_with_connected_entity\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m as short as possible. You must strictly respond in the given JSON format without any additional explanation or commentary. If you cannot generate the correct format, return an empty JSON array []. The text is: \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(text_list)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m; The entities are: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjson\u001b[38;5;241m.\u001b[39mdumps(entity_list)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Get the response from the model\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m response, history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mchat(tokenizer, \u001b[38;5;28;01mNone\u001b[39;00m, question, generation_config, history\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, return_history\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Append the response to the all_responses list\u001b[39;00m\n\u001b[1;32m     51\u001b[0m all_responses\u001b[38;5;241m.\u001b[39mappend(response)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the JSON file\n",
    "with open('./DR_ORI/dr_subsection_diagnosis_with_umls.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Initialize an empty list to hold the updated data with knowledge graph\n",
    "updated_data = []\n",
    "\n",
    "# Initialize a list to collect all model responses\n",
    "all_responses = []\n",
    "\n",
    "# Iterate over diseases and elements\n",
    "for disease_data in data:\n",
    "    disease_name = disease_data.get(\"disease\")\n",
    "\n",
    "    # Extract UMLS entities from the main level\n",
    "    umls_entities = disease_data.get(\"umls_entities\", [])\n",
    "    entity_list = umls_entities  # Include the entire \"umls_entities\" objects\n",
    "\n",
    "    # Extract information from elements\n",
    "    for element in disease_data.get(\"elements\", []):\n",
    "        # Extract UMLS entities from the element level\n",
    "        element_umls_entities = element.get(\"umls_entities\", [])\n",
    "        entity_list.extend(element_umls_entities)  # Include entire \"umls_entities\" objects\n",
    "\n",
    "        # Extract clinical signs\n",
    "        for sign_key, sign_value in element.get(\"clinical_signs\", {}).items():\n",
    "            # Extract 'manifested_as' text from clinical signs (without 'defined_by' text)\n",
    "            text_list = []\n",
    "\n",
    "            manifested_as_text = sign_value.get(\"manifested_as\")\n",
    "            if manifested_as_text:\n",
    "                text_list.append(manifested_as_text)\n",
    "\n",
    "            # Extract UMLS entities from clinical signs\n",
    "            clinical_sign_umls_entities = sign_value.get(\"umls_entities\", [])\n",
    "            entity_list.extend(clinical_sign_umls_entities)  # Include entire \"umls_entities\" objects\n",
    "\n",
    "            # Extract 'img_sample' and 'img_caption' from clinical signs\n",
    "            img_sample = sign_value.get(\"img_sample\", [])\n",
    "            img_caption = sign_value.get(\"img_caption\", [])\n",
    "\n",
    "            # Formulate the question with full UMLS entities\n",
    "            question = f\"\"\"You are an experienced ophthalmologist. You are given a text about ophthalmic diseases and some entities in the text. Your task is to select only the single most relevant entity directly associated with ophthalmic diseases from the entities that have the same \\\"name\\\" and create an ophthalmology knowledge graph according to these entities. The knowledge graph should only include nodes that represent significant ophthalmic entities such as diseases, key symptoms, clinical signs, treatments, anatomical structures specifically related to ophthalmic conditions, and exclude non-specific descriptors such as general fluid accumulation or other non-specific medical terms. The relationships and actions between these nodes are represented as edges. You will respond with a knowledge graph in the given JSON format: [{{\\\"entity\\\" : \\\"Entity_name\\\", \\\"cui\\\": \\\"Entity_CUI\\\", \\\"connections\\\" : [{{\\\"entity\\\" : \\\"Connected_entity\\\", \\\"cui\\\": \\\"Entity_CUI\\\", \\\"relationship\\\" : \\\"Relationship_with_connected_entity\\\"}}, {{\\\"entity\\\" : \\\"Connected_entity\\\", \\\"cui\\\": \\\"Entity_CUI\\\", \\\"relationship\\\" : \\\"Relationship_with_connected_entity\\\"}}]}}]. Keep the \\\"Relationship_with_connected_entity\\\" as short as possible. You must strictly respond in the given JSON format without any additional explanation or commentary. If you cannot generate the correct format, return an empty JSON array []. The text is: \\\"{'. '.join(text_list)}\\\"; The entities are: {json.dumps(entity_list)}.\"\"\"\n",
    "\n",
    "            # Get the response from the model\n",
    "            response, history = model.chat(tokenizer, None, question, generation_config, history=None, return_history=False)\n",
    "\n",
    "            # Append the response to the all_responses list\n",
    "            all_responses.append(response)\n",
    "\n",
    "            # Convert the response into a JSON object\n",
    "            try:\n",
    "                knowledge_graph = json.loads(response)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(\"JSON decoding error:\", e)\n",
    "                print(\"Response saved to response_output.txt for inspection.\")\n",
    "                knowledge_graph = []  # Fallback to an empty list if parsing fails\n",
    "\n",
    "            # Update each entity in the knowledge graph to include \"name\" if missing\n",
    "            for entity in knowledge_graph:\n",
    "                if \"name\" not in entity or not entity[\"name\"]:\n",
    "                    # Find the corresponding entity in the UMLS entities list\n",
    "                    matching_entity = next((e for e in entity_list if e[\"entity\"] == entity[\"entity\"]), None)\n",
    "                    if matching_entity:\n",
    "                        entity[\"name\"] = matching_entity.get(\"name\", \"\")\n",
    "\n",
    "            # Create a new dictionary for the updated clinical sign\n",
    "            updated_clinical_sign = {\n",
    "                \"Knowledge_Graph\": knowledge_graph,\n",
    "                \"img_sample\": img_sample,\n",
    "                \"img_caption\": img_caption\n",
    "            }\n",
    "\n",
    "            # Include disease and optionally sub_disease in the output\n",
    "            sub_disease_name = element.get(\"sub_disease\", None)\n",
    "            clinical_sign_entry = {\n",
    "                \"disease\": disease_name,  # Always include the disease name\n",
    "                \"sign_name\": sign_key,\n",
    "                \"details\": updated_clinical_sign\n",
    "            }\n",
    "            if sub_disease_name:\n",
    "                clinical_sign_entry[\"sub_disease\"] = sub_disease_name  # Include sub_disease if present\n",
    "\n",
    "            # Update the clinical sign information\n",
    "            disease_data.setdefault(\"clinical_signs_with_knowledge_graph\", []).append(clinical_sign_entry)\n",
    "\n",
    "    # Add the updated disease data to the updated data list\n",
    "    updated_data.append(disease_data)\n",
    "\n",
    "# Save the updated JSON structure with knowledge graphs\n",
    "with open('./DR_ORI/updated_dr_subsection_diagnosis_with_knowledge_graph_nomermory.json', 'w') as f:\n",
    "    json.dump(updated_data, f, indent=4)\n",
    "\n",
    "# Save all collected responses to a text file for inspection\n",
    "with open('./DR_ORI/response_output_nomermory.txt', 'w') as response_file:\n",
    "    response_file.write('\\n'.join(all_responses))\n",
    "\n",
    "print(\"Knowledge graphs have been successfully added to the JSON file with img_sample, img_caption, and entity details retained.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = ['<image>\\nDescribe the image in detail.'] * len(num_patches_list)\n",
    "responses = model.batch_chat(tokenizer, None,\n",
    "                             num_patches_list=None,\n",
    "                             questions=questions,\n",
    "                             generation_config=generation_config)\n",
    "for question, response in zip(questions, responses):\n",
    "    print(f'User: {question}\\nAssistant: {response}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.19 ('internvl1.2': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "vscode": {
   "interpreter": {
    "hash": "fde15475963225456dfab0c41463499d99506d7b0d05094f0883b07798e37642"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
